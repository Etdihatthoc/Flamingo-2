# Model Overview

## Description:
Audio Flamingo 2 is a state-of-the-art Audio-Language Model (ALM) designed for:

- Open-ended audio question answering (AQA).
- Expert audio reasoning.
- Long-context audio comprehension.

We introduce innovations in synthetic training data generation, architecture, audio representation learning, and training strategies. Extensive evaluations confirm its effectiveness, setting new benchmarks in audio understanding and reasoning tasks. 

**This model is for non-commercial research purposes only.**

<center><img src="https://huggingface.co/nvidia/audio-flamingo-2/resolve/main/assets/af2_radar.png" width="400"></center>

<br>

<center><img src="https://huggingface.co/nvidia/audio-flamingo-2/resolve/main/assets/af2_arch.png" width="800"></center>

## License / Terms of Use

The models are under the NVIDIA OneWay Noncommercial License. They are also subject to the [Qwen Research license](https://huggingface.co/Qwen/Qwen2.5-3B/blob/main/LICENSE) and the [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI.

## Deployment Geography
Global.

## Use Case
Researchers and developers in the machine learning area are expected to use this model for various purposes including synthetic caption generation, audio labeling, and downstream audio-related applications. 

## Release Date
[03/06/2025]


## References:
* [Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning](https://arxiv.org/abs/2503.03983)
* [Project Page](https://github.com/NVIDIA/audio-flamingo)  
* [Demo Website](https://research.nvidia.com/labs/adlr/AF2/)

## Model Architecture:
**Architecture Type:** Transformer  
**Network Architecture:** Audio Flamingo 2  

Audio Flamingo 2, similar to Audio Flamingo, follows a Flamingo-style architecture with a frozen audio feature extractor, trainable transformation layers, gated cross-attention layers, and language model layers.

## Input:
**Input Types:** Audio, Text  
**Input Format:** WAV/MP3/FLAC, String  
**Input Parameters:** Both string and audio are 1D.  
**Other Properties Related to Input:**  
&nbsp;&nbsp;&nbsp;&nbsp;**Maximum Audio Input Length:** 5 minutes  
&nbsp;&nbsp;&nbsp;&nbsp;**Maximum Text Input Length:** 512 tokens


## Output:
**Output Type:** Text  
**Output Format:** String  
**Output Parameters:** 1D.  
**Other Properties Related to Input:**   
&nbsp;&nbsp;&nbsp;&nbsp;**Maximum Text Output Length:** 512 tokens


## Software Integration:
**Runtime Engine:** PyTorch  

**Supported Hardware:**
* NVIDIA Ampere  
* NVIDIA Hopper  

**Supported OS:**
* Linux  

## Model Version:
* v2.0  

---

## Training, Testing, and Evaluation Datasets:

### Training Dataset:
Audio Flamingo 2 is trained using **publicly available** datasets under various licenses, with the most restricted ones being non-commercial/research-only. The dataset includes diverse audio types: speech, environmental sounds, and music.

All data collection methods are Human. The labeling methods are marked in columns after each dataset.

#### General Sound:
* AudioSkills (Automated)
* LongAudio  (Automated)
* [WavCaps](https://github.com/XinhaoMei/WavCaps) (Automated)
* [MACS](https://zenodo.org/records/5114771) (Human)
* [SoundDescs](https://github.com/akoepke/audio-retrieval-benchmark) (Human)
* [Clotho-v2](https://github.com/audio-captioning/clotho-dataset/tree/master) (Human)
* [WavText5K](https://github.com/microsoft/WavText5K) (Human)
* [Clotho-AQA](https://zenodo.org/records/6473207) (Human)
* [Open-AQA](https://github.com/YuanGongND/ltu?tab=readme-ov-file)  (Automated)
* [Salmonn AQA](https://github.com/bytedance/SALMONN/tree/main)  (Automated)
* [Audio Entailment](https://github.com/microsoft/AudioEntailment)(Automated)
* [CompA](https://github.com/Sreyan88/CompA)  (Automated)
* [AudioSet](https://research.google.com/audioset/download.html)  (Human)
* [FSD50k](https://zenodo.org/records/4060432)  (Human)
* [CochlScene](https://github.com/cochlearai/cochlscene)  (Human)
* [NonSpeech7K](https://zenodo.org/records/6967442)  (Human)
* [Chime-Home](https://code.soundsoftware.ac.uk/projects/chime-home-dataset-annotation-and-baseline-evaluation-code)  (Human)
* [Sonyc-UST](https://zenodo.org/records/3966543)  (Human)

#### Music:
* [LP-MusicCaps](https://github.com/seungheondoh/lp-music-caps)  (Automated)
* [MusicQA](https://github.com/shansongliu/MU-LLaMA?tab=readme-ov-file)  (Automated)
* [MusicAVQA](https://gewu-lab.github.io/MUSIC-AVQA/)  (Human)
* [MusicBench](https://huggingface.co/datasets/amaai-lab/MusicBench)  (Automated)
* [Mu-LLAMA](https://github.com/shansongliu/MU-LLaMA)  (Automated)
* [NSynth](https://magenta.tensorflow.org/datasets/nsynth)  (Human)
* [MTG-Jamendo](https://github.com/MTG/mtg-jamendo-dataset)  (Human)
* [FMA](https://github.com/mdeff/fma)  (Human)
* [MusDB-HQ](https://zenodo.org/records/3338373)  (Human)

#### Speech:
* [MSP-Podcast](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Podcast.html)  (Human)
* [JL-Corpus](https://github.com/tli725/JL-Corpus)  (Human)
* [MELD](https://github.com/declare-lab/MELD)  (Human)
* [Tess](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)  (Human)
* [OMGEmotion](https://github.com/knowledgetechnologyuhh/OMGEmotionChallenge)  (Human)
* [Emov-DB](https://github.com/numediart/EmoV-DB)  (Human)

---

### Evaluation Dataset:
Audio Flamingo 2 is evaluated on the test split of the following datasets.

* [ClothoAQA](https://zenodo.org/records/6473207)  (Human)
* [MusicAVQA](https://gewu-lab.github.io/MUSIC-AVQA/)  (Human)
* [Clotho-v2](https://github.com/audio-captioning/clotho-dataset/tree/master)  (Human)
* [FSD50k](https://zenodo.org/records/4060432)  (Human)
* [CochlScene](https://github.com/cochlearai/cochlscene)  (Human)
* [NonSpeech7K](https://zenodo.org/records/6967442)  (Human)
* [NSynth](https://magenta.tensorflow.org/datasets/nsynth)  (Human)
* [AudioCaps](https://github.com/cdjkim/audiocaps)  (Human)
* [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)  (Human)
* [Ravdess](https://zenodo.org/records/1188976)  (Human)
* [US8K](https://urbansounddataset.weebly.com/urbansound8k.html)  (Human)
* [GTZAN](https://www.tensorflow.org/datasets/catalog/gtzan)  (Human)
* [Medley-solos-DB](https://zenodo.org/records/3464194)  (Human)
* [MMAU](https://github.com/Sakshi113/mmau/tree/main)  (Automated)
* [Audio Entailment](https://github.com/microsoft/AudioEntailment)(Automated)
* [CompA-R-test](https://github.com/Sreyan88/GAMA)  (Automated)
* [MuchoMusic](https://zenodo.org/records/12709974)  (Automated)
* [Open-AQA](https://github.com/YuanGongND/ltu?tab=readme-ov-file)(Automated)
* [MusicInstruct](https://huggingface.co/datasets/m-a-p/Music-Instruct)  (Automated)
* [MusicQA](https://huggingface.co/datasets/mu-llama/MusicQA)  (Automated)
* [CMM Hallucination](https://huggingface.co/datasets/DAMO-NLP-SG/CMM)(Human)

---

## Inference:

**Engine:** HuggingFace Transformers  
**Test Hardware:** NVIDIA A100 80GB  

## Ethical Considerations:
NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  
Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).


## Acknowledgements
**Built with Qwen**